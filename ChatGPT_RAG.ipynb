{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Setup environment"
      ],
      "metadata": {
        "id": "zhXPS75JocTJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlI04lSCl3T9"
      },
      "outputs": [],
      "source": [
        "!pip install openai\n",
        "!pip install langchain chromadb langchainhub langchain-openai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup your chatgpt key"
      ],
      "metadata": {
        "id": "kLzteE_ukVBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Prompt the user for their API key\n",
        "api_key = input(\"Please enter your OpenAI API key: \")\n",
        "\n",
        "# Store the API key in the environment variable\n",
        "os.environ[\"OPENAI_API_KEY\"] = api_key"
      ],
      "metadata": {
        "id": "TCsMLVeVkUot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare the document. For this example, we are using the LibGuide https://guides.nyu.edu/data\n",
        "\n",
        "Check here for more about this workflow: https://python.langchain.com/docs/use_cases/question_answering/"
      ],
      "metadata": {
        "id": "GcrQCO0DCxL-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import WebBaseLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "loader = WebBaseLoader([\n",
        "    'https://guides.nyu.edu/data/home',\n",
        "    'https://guides.nyu.edu/data/llm',\n",
        "    'https://guides.nyu.edu/data/llm-overview',\n",
        "    'https://guides.nyu.edu/data/llm-bias',\n",
        "    'https://guides.nyu.edu/data/llm-research-creative-use',\n",
        "    'https://guides.nyu.edu/data/chatgpt',\n",
        "    'https://guides.nyu.edu/data/chatgpt-research',\n",
        "    'https://guides.nyu.edu/data/chatgpt-developer',\n",
        "    'https://guides.nyu.edu/data/chatgpt-visual-design',\n",
        "    'https://guides.nyu.edu/data/chatgpt-campus-life',\n",
        "    'https://guides.nyu.edu/data/emerging-ai',\n",
        "    'https://guides.nyu.edu/data/chatbots',\n",
        "    'https://guides.nyu.edu/data/ai-image-generation',\n",
        "    'https://guides.nyu.edu/data/ai-local',\n",
        "    'https://guides.nyu.edu/data/ai-teaching-and-learning',\n",
        "    'https://guides.nyu.edu/data/ai-citations',\n",
        "    'https://guides.nyu.edu/data/generative-ai',\n",
        "    'https://guides.nyu.edu/data/ai-governance',\n",
        "    'https://guides.nyu.edu/data/ai-business',\n",
        "    'https://guides.nyu.edu/data/css'\n",
        "])\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=250, chunk_overlap=0)\n",
        "splits = text_splitter.split_documents(loader.load())"
      ],
      "metadata": {
        "id": "n6Y2eq33CzBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize embedding LLM\n",
        "\n"
      ],
      "metadata": {
        "id": "Uz5fpNvfH7oB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "llm_embedding = OpenAIEmbeddings()"
      ],
      "metadata": {
        "id": "2qfGPT-jIMCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create embeddings, this section will take a long time. You can also use the pre-created vectorstore if you skip to the next section."
      ],
      "metadata": {
        "id": "HnSZftLSkDym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "vectorstore = Chroma.from_documents(documents=splits, embedding=llm_embedding)\n",
        "retriever = vectorstore.as_retriever()"
      ],
      "metadata": {
        "id": "tarIlXrLH5FA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check VectorStore"
      ],
      "metadata": {
        "id": "-yym0u0WksdR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever.get_relevant_documents(\"ChatGPT\")"
      ],
      "metadata": {
        "id": "E0Km2NZPksKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare LLM"
      ],
      "metadata": {
        "id": "EZRttQy0pzt9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "temperature = 0\n",
        "max_tokens = 512"
      ],
      "metadata": {
        "id": "qh_sMzlfKoDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4-turbo-preview\", temperature=temperature, max_tokens=max_tokens, streaming=True)"
      ],
      "metadata": {
        "id": "r0UqTZeeqVz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare RAG Chain"
      ],
      "metadata": {
        "id": "Z_gx52TTKJeJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import hub\n",
        "\n",
        "rag_prompt = hub.pull(\"rlm/rag-prompt\")"
      ],
      "metadata": {
        "id": "PS1ZAL6GKEm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "\n",
        "rag_chain = {\"context\": retriever, \"question\": RunnablePassthrough()} | rag_prompt | llm"
      ],
      "metadata": {
        "id": "SP91cPpGKInU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "def infer_rag(text):\n",
        "\n",
        "    logging.info(\"Creating answer for the question:\" + text)\n",
        "\n",
        "    # response = rag_chain.invoke(text)\n",
        "\n",
        "    # logging.info(response)\n",
        "\n",
        "    # return response\n",
        "\n",
        "    response_iterator = rag_chain.stream(text)\n",
        "\n",
        "    return response_iterator"
      ],
      "metadata": {
        "id": "_bw4ZWjpohc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "def infer_llm(text):\n",
        "\n",
        "    logging.info(\"Trying to answer the question:\" + text)\n",
        "\n",
        "    # response = llm(prompt=text)\n",
        "\n",
        "    # # response_content = response[\"choices\"][0][\"text\"]\n",
        "\n",
        "    # logging.info(response)\n",
        "\n",
        "    # return response\n",
        "\n",
        "    response_iterator = llm.stream(text)\n",
        "\n",
        "    return response_iterator"
      ],
      "metadata": {
        "id": "6b2g8-d_nXF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "import time\n",
        "\n",
        "# Text widget for input\n",
        "input_text = widgets.Textarea(\n",
        "    value='',\n",
        "    placeholder='Question',\n",
        "    description='Input:',\n",
        "    disabled=False,\n",
        "    layout={'height': '100px', 'width': '600px'}\n",
        ")\n",
        "\n",
        "# Button to execute function\n",
        "execute_button = widgets.Button(description=\"Generate\")\n",
        "\n",
        "progress_bar = widgets.IntProgress(\n",
        "    value=0,\n",
        "    min=0,\n",
        "    max=100,\n",
        "    description='Progress:',\n",
        "    bar_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
        "    orientation='horizontal'\n",
        ")\n",
        "\n",
        "\n",
        "# Textarea widgets to display the results\n",
        "output1 = widgets.Textarea(\n",
        "    value='',\n",
        "    placeholder='Answer with RAG will be displayed here',\n",
        "    description='Answer with RAG:',\n",
        "    disabled=True,\n",
        "    layout={'height': '200px', 'width': '600px'}  # Adjust height and width as needed\n",
        ")\n",
        "\n",
        "output2 = widgets.Textarea(\n",
        "    value='',\n",
        "    placeholder='Answer without RAG will be displayed here',\n",
        "    description='Answer without RAG:',\n",
        "    disabled=True,\n",
        "    layout={'height': '200px', 'width': '600px'}  # Adjust height and width as needed\n",
        ")\n",
        "\n",
        "# Function to be executed\n",
        "def execute_function(change):\n",
        "    progress_bar.value = 0\n",
        "    progress_bar.max = 3  # Set this to a high number for a smooth progress bar\n",
        "    progress_bar.style.bar_color = 'blue'\n",
        "    display(progress_bar)\n",
        "\n",
        "    # Step 1: You might want to update progress bar here\n",
        "    progress_bar.value += 1\n",
        "\n",
        "    # output_text_rag = infer_rag(input_text.value)\n",
        "\n",
        "    # output1.value = output_text_rag\n",
        "\n",
        "    for completion in infer_rag(input_text.value):\n",
        "      completion_text = completion.content\n",
        "      # print(completion)\n",
        "      output1.value += completion_text\n",
        "\n",
        "    # Step 2: Another update to the progress bar\n",
        "    progress_bar.value += 1\n",
        "\n",
        "    # output_text_llm = infer_llm(input_text.value)\n",
        "\n",
        "    # output2.value = output_text_llm\n",
        "\n",
        "    for completion in infer_llm(input_text.value):\n",
        "      completion_text = completion.content\n",
        "      # print(completion)\n",
        "      output2.value += completion_text\n",
        "\n",
        "    # Step 3: Final update to the progress bar\n",
        "    progress_bar.value += 1\n",
        "    progress_bar.bar_style = 'success'\n",
        "\n",
        "# Attach the function to the button\n",
        "execute_button.on_click(execute_function)\n",
        "\n",
        "# Display widgets\n",
        "display(input_text, execute_button, output1, output2)\n"
      ],
      "metadata": {
        "id": "no1IPyzyte0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example\n",
        "\n",
        "What is LLM?"
      ],
      "metadata": {
        "id": "OuQWaOrm1YWN"
      }
    }
  ]
}
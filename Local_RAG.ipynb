{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# MAKE SURE YOU ARE RUNNING THE GPU RUNTIME.\n",
        "\n",
        "You can change to the GPU runtime by Runtime > Change Runtime Type > T4 GPU."
      ],
      "metadata": {
        "id": "jRjWqeZ7AA88"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confirm you have a GPU by running the below code."
      ],
      "metadata": {
        "id": "tVLg4M7wARzo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "Xb2CW-yhAP3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup environment"
      ],
      "metadata": {
        "id": "zhXPS75JocTJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlI04lSCl3T9"
      },
      "outputs": [],
      "source": [
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python\n",
        "!pip install langchain chromadb langchainhub\n",
        "!pip install --upgrade --quiet  sentence_transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download model. For this example, we are using Mistral 7B Instruct."
      ],
      "metadata": {
        "id": "hdyUWuUfoak5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q6_K.gguf"
      ],
      "metadata": {
        "id": "KbEFtrCynIHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# If you already have the model downloaded, or want to save it run the following sections."
      ],
      "metadata": {
        "id": "2mBPCE6glSyC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Enable Drive Connection by Clicking the Button at Files at tab."
      ],
      "metadata": {
        "id": "wRdKk3zvldmy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save Model to Drive"
      ],
      "metadata": {
        "id": "N8IbZ9KGlaeW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp ./mistral-7b-instruct-v0.1.Q6_K.gguf \"/content/drive/MyDrive/LLAMA Models/\""
      ],
      "metadata": {
        "id": "_NG-6VpOlSel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download Model from Drive"
      ],
      "metadata": {
        "id": "YIQDqEIxlrwz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp \"/content/drive/MyDrive/LLAMA Models/mistral-7b-instruct-v0.1.Q6_K.gguf\" ./mistral-7b-instruct-v0.1.Q6_K.gguf"
      ],
      "metadata": {
        "id": "tgTz1PDnlrht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare the document. For this example, we are using the LibGuide https://guides.nyu.edu/data\n",
        "\n",
        "Check here for more about this workflow: https://python.langchain.com/docs/use_cases/question_answering/"
      ],
      "metadata": {
        "id": "GcrQCO0DCxL-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import WebBaseLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "loader = WebBaseLoader([\n",
        "    'https://guides.nyu.edu/data/home',\n",
        "    'https://guides.nyu.edu/data/llm',\n",
        "    'https://guides.nyu.edu/data/llm-overview',\n",
        "    'https://guides.nyu.edu/data/llm-bias',\n",
        "    'https://guides.nyu.edu/data/llm-research-creative-use',\n",
        "    'https://guides.nyu.edu/data/chatgpt',\n",
        "    'https://guides.nyu.edu/data/chatgpt-research',\n",
        "    'https://guides.nyu.edu/data/chatgpt-developer',\n",
        "    'https://guides.nyu.edu/data/chatgpt-visual-design',\n",
        "    'https://guides.nyu.edu/data/chatgpt-campus-life',\n",
        "    'https://guides.nyu.edu/data/emerging-ai',\n",
        "    'https://guides.nyu.edu/data/chatbots',\n",
        "    'https://guides.nyu.edu/data/ai-image-generation',\n",
        "    'https://guides.nyu.edu/data/ai-local',\n",
        "    'https://guides.nyu.edu/data/ai-teaching-and-learning',\n",
        "    'https://guides.nyu.edu/data/ai-citations',\n",
        "    'https://guides.nyu.edu/data/generative-ai',\n",
        "    'https://guides.nyu.edu/data/ai-governance',\n",
        "    'https://guides.nyu.edu/data/ai-business',\n",
        "    'https://guides.nyu.edu/data/css'\n",
        "])\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=250, chunk_overlap=0)\n",
        "splits = text_splitter.split_documents(loader.load())"
      ],
      "metadata": {
        "id": "n6Y2eq33CzBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize embedding LLM, it will download the model\n",
        "\n"
      ],
      "metadata": {
        "id": "Uz5fpNvfH7oB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "\n",
        "llm_embedding = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "id": "2qfGPT-jIMCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create embeddings"
      ],
      "metadata": {
        "id": "HnSZftLSkDym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "vectorstore = Chroma.from_documents(documents=splits, embedding=llm_embedding, persist_directory=\"./libguide\")\n",
        "retriever = vectorstore.as_retriever()"
      ],
      "metadata": {
        "id": "tarIlXrLH5FA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check VectorStore"
      ],
      "metadata": {
        "id": "-yym0u0WksdR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever.get_relevant_documents(\"ChatGPT\")"
      ],
      "metadata": {
        "id": "E0Km2NZPksKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare LLM"
      ],
      "metadata": {
        "id": "EZRttQy0pzt9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "temperature = 0.1\n",
        "top_k = 40\n",
        "top_p = 0.9\n",
        "max_tokens = 2048"
      ],
      "metadata": {
        "id": "qh_sMzlfKoDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import LlamaCpp\n",
        "llm = LlamaCpp(model_path=\"./mistral-7b-instruct-v0.1.Q6_K.gguf\", n_ctx=32768, use_mlock=True, n_gpu_layers=33, temperature=temperature, top_k=top_k, top_p=top_p, max_tokens=max_tokens, streaming=True)"
      ],
      "metadata": {
        "id": "r0UqTZeeqVz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare RAG Chain"
      ],
      "metadata": {
        "id": "Z_gx52TTKJeJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import hub\n",
        "\n",
        "rag_prompt = hub.pull(\"rlm/rag-prompt\")"
      ],
      "metadata": {
        "id": "PS1ZAL6GKEm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "\n",
        "rag_chain = {\"context\": retriever, \"question\": RunnablePassthrough()} | rag_prompt | llm"
      ],
      "metadata": {
        "id": "SP91cPpGKInU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "def infer_rag(text):\n",
        "\n",
        "    logging.info(\"Creating answer for the question:\" + text)\n",
        "\n",
        "    # response = rag_chain.invoke(text)\n",
        "\n",
        "    # logging.info(response)\n",
        "\n",
        "    # return response\n",
        "\n",
        "    response_iterator = rag_chain.stream(text)\n",
        "\n",
        "    return response_iterator"
      ],
      "metadata": {
        "id": "_bw4ZWjpohc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "def infer_llm(text):\n",
        "\n",
        "    logging.info(\"Trying to answer the question:\" + text)\n",
        "\n",
        "    # response = llm(prompt=text)\n",
        "\n",
        "    # # response_content = response[\"choices\"][0][\"text\"]\n",
        "\n",
        "    # logging.info(response)\n",
        "\n",
        "    # return response\n",
        "\n",
        "    response_iterator = llm.stream(text)\n",
        "\n",
        "    return response_iterator"
      ],
      "metadata": {
        "id": "6b2g8-d_nXF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "import time\n",
        "\n",
        "# Text widget for input\n",
        "input_text = widgets.Textarea(\n",
        "    value='',\n",
        "    placeholder='Question',\n",
        "    description='Input:',\n",
        "    disabled=False,\n",
        "    layout={'height': '100px', 'width': '600px'}\n",
        ")\n",
        "\n",
        "# Button to execute function\n",
        "execute_button = widgets.Button(description=\"Generate\")\n",
        "\n",
        "progress_bar = widgets.IntProgress(\n",
        "    value=0,\n",
        "    min=0,\n",
        "    max=100,\n",
        "    description='Progress:',\n",
        "    bar_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
        "    orientation='horizontal'\n",
        ")\n",
        "\n",
        "\n",
        "# Textarea widgets to display the results\n",
        "output1 = widgets.Textarea(\n",
        "    value='',\n",
        "    placeholder='Answer with RAG will be displayed here',\n",
        "    description='Answer with RAG:',\n",
        "    disabled=True,\n",
        "    layout={'height': '200px', 'width': '600px'}  # Adjust height and width as needed\n",
        ")\n",
        "\n",
        "output2 = widgets.Textarea(\n",
        "    value='',\n",
        "    placeholder='Answer without RAG will be displayed here',\n",
        "    description='Answer without RAG:',\n",
        "    disabled=True,\n",
        "    layout={'height': '200px', 'width': '600px'}  # Adjust height and width as needed\n",
        ")\n",
        "\n",
        "# Function to be executed\n",
        "def execute_function(change):\n",
        "    progress_bar.value = 0\n",
        "    progress_bar.max = 3  # Set this to a high number for a smooth progress bar\n",
        "    progress_bar.style.bar_color = 'blue'\n",
        "    display(progress_bar)\n",
        "\n",
        "    # Step 1: You might want to update progress bar here\n",
        "    progress_bar.value += 1\n",
        "\n",
        "    # output_text_rag = infer_rag(input_text.value)\n",
        "\n",
        "    # output1.value = output_text_rag\n",
        "\n",
        "    for completion in infer_rag(input_text.value):\n",
        "      output1.value += completion\n",
        "\n",
        "    # Step 2: Another update to the progress bar\n",
        "    progress_bar.value += 1\n",
        "\n",
        "    # output_text_llm = infer_llm(input_text.value)\n",
        "\n",
        "    # output2.value = output_text_llm\n",
        "\n",
        "    for completion in infer_llm(input_text.value):\n",
        "      output2.value += completion\n",
        "\n",
        "    # Step 3: Final update to the progress bar\n",
        "    progress_bar.value += 1\n",
        "    progress_bar.bar_style = 'success'\n",
        "\n",
        "# Attach the function to the button\n",
        "execute_button.on_click(execute_function)\n",
        "\n",
        "# Display widgets\n",
        "display(input_text, execute_button, output1, output2)\n"
      ],
      "metadata": {
        "id": "no1IPyzyte0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example\n",
        "\n",
        "What is LLM?"
      ],
      "metadata": {
        "id": "OuQWaOrm1YWN"
      }
    }
  ]
}